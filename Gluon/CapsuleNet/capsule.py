import mxnet.gluon as gluonimport mxnet as mxclass Primarycaps(gluon.HybridBlock):    def __init__(self, **kwargs):        super(Primarycaps , self).__init__(**kwargs)        with self.name_scope():            '''            In the paper, The first layer() -> 256 channel            '''            self.conv1 = gluon.nn.Conv2D(channels=256, kernel_size=(9, 9), strides=(1, 1), activation="relu", use_bias=False)            self.conv2 = gluon.nn.Conv2D(channels=32*8, kernel_size=(9, 9), strides=(2, 2) , use_bias=False)    def squashing(self, F, x, axis=2):        #x -> (batch_size, 32*6*6, 8)        square_sum=x.square().sum(axis=axis, keepdims=True)        sqrt_sum =square_sum.sqrt()        x= F.broadcast_mul(x,square_sum/(1+square_sum)/sqrt_sum)        return x    def hybrid_forward(self, F, x):        x=self.conv1(x) # (batch_size, 256, 20, 20)        x=self.conv2(x) # (batch_size, 256, 6,6)        x=F.reshape(x, (-1,32,8,6,6))        x=F.transpose(x , axes=(0,1,3,4,2))        x=F.reshape(x , (-1,32*6*6,8))        x=self.squashing(F, x, axis=2) # 8D vector        return xclass DigitCaps(gluon.HybridBlock):    def __init__(self, batch_size, **kwargs):        super(DigitCaps, self).__init__(**kwargs)        self.batch_size=batch_size        with self.name_scope():            self.weight = self.params.get('weight', shape=(1,32*6*6,10,8,16),                                          allow_deferred_init=True,differentiable=True)    def hybrid_forward(self, F, x,  weight):        # x -> (batch_size,32*6*6, 1, 8, 1)        x = F.expand_dims(x,axis=2)        x = F.expand_dims(x,axis=4)        x = F.tile(x,reps=(1,1,10,1,1)) #(-1,32*6*6,10,8,1)        weight = F.tile(weight,reps=(self.batch_size,1,1,1,1))        x=F.reshape(x, shape=(-1,8,1))        weight = F.reshape(weight,shape=(-1,8,16))        u_hat = F.batch_dot(weight, x, transpose_a=True)        u_hat = F.reshape(u_hat,shape=(-1, 32*6*6, 10, 16, 1))        return u_hat#routing iterations Consider also when 0class Routing_algorithm(gluon.HybridBlock):    def __init__(self, Routing_Iteration=3, **kwargs):        super(Routing_algorithm , self).__init__(**kwargs)        self.Routing_Iteration = Routing_Iteration    def squashing(self,F, x, axis=3):        #x -> (batch_size,1,10,16,1)        square_sum=x.square().sum(axis=axis, keepdims=True)        sqrt_sum =square_sum.sqrt()        x= F.broadcast_mul(x,square_sum/(1+square_sum)/sqrt_sum)        return x    def hybrid_forward(self ,F , x):        b=F.zeros((1,32*6*6,10,1,1))        # u_hat = x -> (batch_size , 32*6*6, 10, 16, 1)        # b -> (1, 32*6*6, 10, 1, 1)        if self.Routing_Iteration==0:            # Routing algorithm procedure 1            c = F.softmax(b, axis=2)  # (1, 32*6*6, 10, 1, 1)            # Routing algorithm procedure 2            s = F.broadcast_mul(c, x)  # (batch_size, 32*6*6, 10, 16, 1)            s = F.sum(s, axis=1, keepdims=True)  # (batch_size,1,10,16,1)            # Routing algorithm procedure 3            v = self.squashing(F, s, axis=3)  # (batch_size,1,10,16,1)        else: #self.Routing_Iteration>0            for i in range(self.Routing_Iteration):                #Routing algorithm procedure 1                c = F.softmax(b, axis=2)  # (1, 32*6*6, 10, 1, 1)                # Routing algorithm procedure 2                s = F.broadcast_mul(c , x) # (batch_size, 32*6*6, 10, 16, 1)                s = F.sum(s, axis=1, keepdims=True)  # (batch_size,1,10,16,1)                # Routing algorithm procedure 3                v = self.squashing(F, s, axis=3)  # (batch_size,1,10,16,1)                # Routing algorithm procedure 4                v_ = F.tile(v, reps=(1, 32*6*6, 1, 1, 1))  # (batch_size,32*6*6,10,16,1)                x_=F.reshape(x, shape=(-1,16, 1))                v_=F.reshape(v_, shape=(-1,16, 1))                agreement= F.batch_dot(x_, v_, transpose_a=True) # (-1, 1, 1)                agreement = F.reshape(agreement, shape=(-1, 1152, 10, 1, 1))                agreement= F.sum(agreement, axis=0 , keepdims=True) # (1, 1152, 10, 1, 1)                b=b+agreement        return vclass Margin_Loss(gluon.loss.Loss):    def __init__(self, weight=1., batch_axis=0, **kwargs):        super(Margin_Loss, self).__init__(weight, batch_axis, **kwargs)        self.batch_axis=batch_axis    def hybrid_forward(self, F, pred, label):        '''            pred : [batch_size,1, 10, 16, 1] The output from `DigitCaps` layer.            label : target: [batch_size, 10] One-hot MNIST labels.        '''        label = F.one_hot(label,depth=10)        square_sum=pred.square().sum(axis=3, keepdims=True)        sqrt_sum =square_sum.sqrt()        max_left = F.square(F.maximum(0, 0.9-sqrt_sum)).reshape((-1,10))        max_right =F.square(F.maximum(0, sqrt_sum-0.1)).reshape((-1,10))        loss=label*max_left+0.5*(1-label)*max_right        return F.mean(loss, axis=self.batch_axis, exclude=True)class Reconstruction_Layer(gluon.HybridBlock):    def __init__(self, **kwargs):        super(Reconstruction_Layer, self).__init__(**kwargs)        with self.name_scope():            self.fnn1=gluon.nn.Dense(512, activation="relu")            self.fnn2=gluon.nn.Dense(1024, activation="relu")            self.fnn3=gluon.nn.Dense(784, activation="sigmoid")    def hybrid_forward(self, F, x, label):        '''            x : [batch_size,1, 10, 16, 1] The output from `DigitCaps` layer.            label : [batch_size,]        '''        '''        In the paper,                4.1 Reconstruction as a regularization method            We use an additional reconstruction loss to encourage the digit capsules to encode the instantiation            parameters of the input digit. During training, we mask out all but the activity vector of the correct            digit capsule. Then we use this activity vector to reconstruct. The output of the digit capsule is fed            into a decoder consisting of 3 fully connected layers that model the pixel intensities as described in            Fig. 2. We minimize the sum of squared differences between the outputs of the logistic units and the            pixel intensities. We scale down this reconstruction loss by 0:0005 so that it does not dominate the            margin loss during training. As illustrated in Fig. 3 the reconstructions from the 16D output of the            CapsNet are robust while keeping only important details.                    <masking process> using tile, pick function        '''        label=F.reshape(label, shape=(-1,1,1,1))        label=F.tile(label, reps=(1,1,16,1))        mask_x=F.pick(x, index=label, axis=2, keepdims=True)        x = F.reshape(mask_x, shape=(-1,16))        x = self.fnn1(x)        x = self.fnn2(x)        x = self.fnn3(x)        return x