import mxnet as mxdef squashing(x,axis=None):    square_sum = x.square().sum(axis=axis, keepdims=True)    sqrt_sum = square_sum.sqrt()    x = mx.sym.broadcast_mul(x, square_sum / (1 + square_sum) / sqrt_sum)    return xdef capsule(reconstruction=True,routing_iteration=3,batch_size=128):    data = mx.sym.Variable('data')    label = mx.sym.Variable('label')    with mx.name.Prefix("Convolution"):        conv1 = mx.sym.Convolution(data=data, kernel=(9, 9), stride=(1, 1), num_filter=256, no_bias=True)        relu1 = mx.sym.Activation(data=conv1, act_type="relu")  # (batch_size, 256, 20, 20)        Primary = mx.sym.Convolution(data=relu1, kernel=(9, 9), stride=(2, 2), num_filter=32*8, no_bias=True) # (batch_size, 256, 6,6)    with mx.name.Prefix("PrimaryCaps"):        Primary=mx.sym.reshape(Primary,(-1,32,8,6,6))        Primary=mx.sym.transpose(Primary , axes=(0,1,3,4,2))        Primary=mx.sym.reshape(Primary , (-1,32*6*6,8))        Primary=squashing(Primary, axis=2) # 8D vector (batch_size,32*6*6, 1, 8, 1)    with mx.name.Prefix("DigitCaps"):        weight=mx.sym.Variable("weight",shape=(1,32*6*6,10,8,16))        Primary=mx.sym.expand_dims(Primary ,axis=2)        Primary=mx.sym.expand_dims(Primary ,axis=4)        Primary=mx.sym.tile(Primary ,reps=(1,1,10,1,1)) #(-1,32*6*6,10,8,1)        weight = mx.sym.tile(weight, reps=(batch_size,1,1,1,1))        Primary=mx.sym.reshape(Primary, shape=(-1,8,1))        weight = mx.sym.reshape(weight,shape=(-1,8,16))        u_hat = mx.sym.batch_dot(weight, Primary, transpose_a=True)        u_hat = mx.sym.reshape(u_hat,shape=(-1, 32*6*6, 10, 16, 1))    #Routing algorithm    b = mx.sym.zeros((1, 32 * 6 * 6, 10, 1, 1))    # u_hat -> (batch_size , 32*6*6, 10, 16, 1)    # b -> (1, 32*6*6, 10, 1, 1)    if routing_iteration == 0:        # Routing algorithm procedure 1        c = mx.sym.softmax(b, axis=2)  # (1, 32*6*6, 10, 1, 1)        # Routing algorithm procedure 2        s = mx.sym.broadcast_mul(c, u_hat)  # (batch_size, 32*6*6, 10, 16, 1)        s = mx.sym.sum(s, axis=1, keepdims=True)  # (batch_size,1,10,16,1)        # Routing algorithm procedure 3        v = squashing(s, axis=3)  # (batch_size,1,10,16,1)    else:  # self.Routing_Iteration>0        for i in range(routing_iteration):            # Routing algorithm procedure 1            c = mx.sym.softmax(b, axis=2)  # (1, 32*6*6, 10, 1, 1)            # Routing algorithm procedure 2            s = mx.sym.broadcast_mul(c, u_hat)  # (batch_size, 32*6*6, 10, 16, 1)            s = mx.sym.sum(s, axis=1, keepdims=True)  # (batch_size,1,10,16,1)            # Routing algorithm procedure 3            v = squashing(s, axis=3)  # (batch_size,1,10,16,1)            # Routing algorithm procedure 4            v_ = mx.sym.tile(v, reps=(1, 32 * 6 * 6, 1, 1, 1))  # (batch_size,32*6*6,10,16,1)            x_ = mx.sym.reshape(u_hat, shape=(-1, 16, 1))            v_ = mx.sym.reshape(v_, shape=(-1, 16, 1))            agreement = mx.sym.batch_dot(x_, v_, transpose_a=True)  # (-1, 1, 1)            agreement = mx.sym.reshape(agreement, shape=(-1, 1152, 10, 1, 1))            agreement = mx.sym.sum(agreement, axis=0, keepdims=True)  # (1, 1152, 10, 1, 1)            b = b + agreement    capsule_output=v    #margin_loss    m_label = mx.sym.one_hot(label, depth=10)    square_sum = capsule_output.square().sum(axis=3, keepdims=True)    sqrt_sum = square_sum.sqrt()    max_left = mx.sym.square(mx.sym.maximum(0, 0.9 - sqrt_sum)).reshape((-1, 10))    max_right = mx.sym.square(mx.sym.maximum(0, sqrt_sum - 0.1)).reshape((-1, 10))    margin_loss = m_label * max_left + 0.5 * (1 - m_label) * max_right    margin_loss= mx.sym.mean(margin_loss, axis=0, exclude=True)    if reconstruction:        with mx.name.Prefix("Reconstruction"):            '''            In the paper,            4.1 Reconstruction as a regularization method                We use an additional reconstruction loss to encourage the digit capsules to encode the instantiation                parameters of the input digit. During training, we mask out all but the activity vector of the correct                digit capsule. Then we use this activity vector to reconstruct. The output of the digit capsule is fed                into a decoder consisting of 3 fully connected layers that model the pixel intensities as described in                Fig. 2. We minimize the sum of squared differences between the outputs of the logistic units and the                pixel intensities. We scale down this reconstruction loss by 0:0005 so that it does not dominate the                margin loss during training. As illustrated in Fig. 3 the reconstructions from the 16D output of the                CapsNet are robust while keeping only important details.            <masking process> using tile, pick function'''                        r_label = mx.sym.reshape(label, shape=(-1, 1, 1, 1))            r_label = mx.sym.tile(r_label, reps=(1, 1, 16, 1))            mask_x = mx.sym.pick(v, index=r_label, axis=2, keepdims=True)            mask_x = mx.sym.reshape(mask_x, shape=(-1, 16))            fnn1 = mx.sym.FullyConnected(data=mask_x, num_hidden=512) # in paper, 512)            fnn1 = mx.sym.Activation(data=fnn1,act_type='relu')            fnn2 = mx.sym.FullyConnected(data=fnn1, num_hidden=1024) #in paper, 1024            fnn2 = mx.sym.Activation(data=fnn2, act_type='relu')            fnn3 = mx.sym.FullyConnected(data=fnn2, num_hidden=784)            fnn3 = mx.sym.Activation(data=fnn3, act_type='sigmoid')        reconstruction_output=fnn3        #reconstruction loss        recon_loss=mx.sym.mean(mx.sym.square(fnn3-data.reshape((batch_size,-1)))/2, axis=0, exclude=True)        #total_loss = margin_loss + reconstruction_loss        total_loss = margin_loss + 0.0005 * recon_loss        return mx.sym.Group([total_loss, capsule_output, reconstruction_output])    else:        return mx.sym.Group([margin_loss, capsule_output])